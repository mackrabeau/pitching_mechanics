RL Roadmap – current status
═══════════════════════════

Step 1 DONE – Inverse-dynamics torque baseline
─────────────────────────────────────────────────
Pre-computed IK trajectory → smoothed → mj_inverse → joint torques.
Output: pitching_mechanics/logs/torques_1623_3.csv (440 rows × 21 joints)

Step 2 DONE – Ball release + strike zone
────────────────────────────────────────────
  - ball_flight.py: compute_release(), check_strike(), compute_reward()
  - Ball speed estimated from hand jc speed × 1.5 wrist ratio
  - Validated: 87.0 mph estimated vs 85.3 mph actual (2% error)
  - Strike check: release height (1.0–2.2m) + forward fraction (≥80%)
  - RL reward = speed_mph + 10 × strike_quality
  - Ball geom added to MJCF model (white sphere on throw_hand)

Step 3 – Gym environment wrapper  ← NEXT
─────────────────────────────────────────
Wrap the simulation as a gymnasium.Env:
  Observation:  joint angles, velocities, time-in-delivery
  Action:       joint torque deltas on top of the reference trajectory
  Reward:       compute_reward(release, strike)  (speed + strike bonus)
  Episode:      reset to fp_10_time pose → simulate → done at MIR_time
  Reset:        restore OBP reference state
  Design:       residual policy learning (perturb the reference, don't start from scratch)

Step 4 – RL training
────────────────────
Algorithm: PPO (Stable Baselines3)
Warmstart: optional behavior cloning on reference trajectory
Constraints: penalize supraphysiological joint angles/torques

Step 5 – Polish
────────────────
Wider replay window, foot–ground contact model, refined anthropometrics,
air drag on ball, wrist DOF for more accurate release direction.

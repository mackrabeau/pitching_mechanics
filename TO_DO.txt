RL Roadmap – current status
═══════════════════════════

Step 1 DONE – Inverse-dynamics torque baseline
─────────────────────────────────────────────────
Pre-computed IK trajectory → smoothed → mj_inverse → joint torques.
Output: pitching_mechanics/logs/torques_1623_3.csv (440 rows × 21 joints)

Step 2 DONE – Ball release + strike zone
────────────────────────────────────────────
  - ball_flight.py: compute_release(), check_strike(), compute_reward()
  - Ball speed estimated from hand jc speed × 1.5 wrist ratio
  - Validated: 87.0 mph estimated vs 85.3 mph actual (2% error)
  - Strike check: release height (1.0–2.2m) + forward fraction (≥80%)
  - RL reward = speed_mph + 10 × strike_quality
  - Ball geom added to MJCF model (white sphere on throw_hand)

Step 3 DONE – Gym environment wrapper
──────────────────────────────────────
  Files: pitch_env.py, trajectory.py (shared IK precomputation)
  Design: residual policy learning — agent outputs small joint-angle
    offsets added to the pre-computed reference IK trajectory.
  Observation space (49-d float32):
    - Joint angles (21 hinge DOFs)
    - Joint angular velocities (21)
    - Hand 3D position + velocity (6)
    - Time-in-delivery (1 scalar, normalized 0→1)
  Action space (21-d float32):
    - Position offsets in [-1, 1], scaled by action_scale (default ±0.15 rad)
    - One per hinge joint, added to reference IK target each timestep
  Physics:
    - Pelvis root driven by mocap body + weld equality (stable kinematic root)
    - Implicit integrator for numerical stability with stiff actuators
    - Position actuators track reference + agent offset
    - 1 kHz physics, 440 steps per episode (0.44s)
  Reward:
    - At BR_time: ball_speed_mph + 10 × strike_quality (via ball_flight.py)
    - Per-step: −w_energy × Σctrl² × dt
    - Per-step: −w_limit × joint-limit proximity penalty (clamped)
  Termination:
    - Truncated: end of replay window (MIR + 0.25s)
    - Terminated: NaN / joints > ±10 rad → −20 penalty
  Verified: gymnasium + SB3 env_checker pass, PPO trains successfully
  Zero-action baseline reward: ~38 (reference tracking only)

Step 4 – RL training  ← NEXT
──────────────────────────
  Algorithm: PPO (Stable Baselines3)
  Training: ≥1M timesteps, parallel envs for throughput
  Warmstart: optional behavior cloning on reference trajectory
  Curriculum:
    - Phase 1: reduced joint set (elbow only) to stabilize early learning
    - Phase 2: add shoulder DOFs
    - Phase 3: full chain (torso + legs)
  Logging: reward per episode, ball velocity, peak joint torques,
    energy expenditure
  Checkpoints: save policy every N episodes for evaluation

Step 5 – Evaluation & visualization
────────────────────────────────────
  - Replay trained policy in MuJoCo viewer
  - Per-episode metrics: release velocity, release angle, peak
    joint torques, mechanical energy
  - Compare learned mechanics vs reference (OBP baseline)

Step 6 – Polish
────────────────
  Wider replay window (wind-up), foot–ground contact model,
  refined anthropometrics (real mass distribution), air drag on ball,
  wrist DOF for more accurate release direction.
